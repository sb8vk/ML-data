{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "authorship_tag": "ABX9TyMJHgkh9ZP4IcuFBMC8fGKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sb8vk/ML/blob/master/Accelerating_the_RAG_Retriever_A_%22Zero_Copy%22_GPU_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**The Context**\n",
        "Most RAG discussions focus heavily on the LLM. However, the bottleneck is often the Retriever, specifically the ingestion, cleaning, and indexing of massive enterprise datasets. If a CPU-based pipeline takes 12 hours to re-index the data, the LLM is always 12 hours out of date."
      ],
      "metadata": {
        "id": "WVL1l5pexznJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**The Approach**\n",
        " We are going to move the \"ETL\" (Extract, Transform, Load) part of RAG onto the GPU using NVIDIA RAPIDS. By keeping data on the GPU, we avoid the latency penalty of moving data back and forth between system RAM and VRAM."
      ],
      "metadata": {
        "id": "5Ud3FesAj9NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What We're Building**\n",
        "\n",
        "Setup: A script to handle Colab's varying driver versions automatically.\n",
        "\n",
        "Ingestion: Compare pandas (CPU) vs cudf (GPU) on a large dataset.\n",
        "\n",
        "Cleaning: Perform regex redaction on millions of rows in parallel.\n",
        "\n",
        "Indexing: Build a vector search index using cuVS (CAGRA)."
      ],
      "metadata": {
        "id": "kWDC3irkkGPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hardware Requirement**\n",
        "Go to Runtime > Change runtime type.\n",
        "\n",
        "Select T4 GPU (or better).\n",
        "\n",
        "Setting up a GPU environment on cloud notebooks can be tricky. Three moving parts that must align:\n",
        "- The NVIDIA Driver (managed by Google)\n",
        "- The CUDA Runtime\n",
        "- The Python Version\n",
        "\n",
        "To solve this reliably, we use the RAPIDS Colab Installer. This detects the specific environment at runtime and fetches the correct pre-compiled wheels, avoiding the memory-crashing \"build from source\" failures common on smaller instances."
      ],
      "metadata": {
        "id": "nG3zVVwFyCf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "hguXWLlyVgGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Auto-Install RAPIDS (The Smart Way)\n",
        "# This script detects your specific Colab config (Driver + Python)\n",
        "# and installs the matching RAPIDS version to prevent conflicts.\n",
        "\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STOP! Please go to 'Runtime > Restart Session' now.\")\n",
        "print(\"This is required to load the new CUDA libraries correctly.\")\n",
        "print(\"Then, proceed directly to Cell 2.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "qatCVsRLHJpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we installed low-level system libraries in Step 1, we had to restart the Python kernel. This gives us a clean slate but wipes out ephemeral packages. Here, we quickly restore sentence-transformers and import our GPU stack (cudf, cupy, cuvs)."
      ],
      "metadata": {
        "id": "0ektTlkUMV9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Restore Dependencies & Import**"
      ],
      "metadata": {
        "id": "6RlDAGqsN_P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Restore Dependencies & Import\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "\n",
        "# 1. Restore Sentence-Transformers (often wiped on reset)\n",
        "try:\n",
        "    import sentence_transformers\n",
        "except ImportError:\n",
        "    print(\"Installing sentence-transformers...\")\n",
        "    !pip install -q sentence-transformers\n",
        "\n",
        "# 2. Import the RAPIDS Stack\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from cuvs.neighbors import cagra\n",
        "\n",
        "print(f\"   Environment Ready!\")\n",
        "print(f\"   RAPIDS cuDF Version: {cudf.__version__}\")\n",
        "print(f\"   GPU Detected: {cp.cuda.runtime.getDeviceCount()} device(s)\")"
      ],
      "metadata": {
        "id": "D4_Nz-FOHSor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Data Generation: Latency vs. Throughput**\n",
        "A common misconception is that GPUs are always faster than CPUs. However, they have a fixed startup cost (initializing the CUDA context).\n",
        "\n",
        "Small Data (100k rows): The CPU often wins because it starts instantly.\n",
        "\n",
        "Big Data (5M rows): The GPU wins because its parallelism amortizes that startup cost.\n",
        "\n",
        "We will generate both sizes to demonstrate this crossover point."
      ],
      "metadata": {
        "id": "GA86f5Uwpns4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Generate Synthetic Data (Small vs. Big)\n",
        "# Define sizes\n",
        "num_rows_small = 100_000\n",
        "num_rows_big   = 5_000_000\n",
        "\n",
        "print(f\"--- Generating Datasets ---\")\n",
        "\n",
        "# --- 1. Small Dataset (100k rows) ---\n",
        "print(f\"Creating Small Dataset ({num_rows_small:,} rows)...\")\n",
        "# We use a simple dictionary structure for speed\n",
        "data_small = {\n",
        "    'id': range(num_rows_small),\n",
        "    'val': np.random.rand(num_rows_small),\n",
        "    'text': [f\"CONFIDENTIAL: Project Alpha data {i} Contact: user{i}@corp.com\" for i in range(num_rows_small)]\n",
        "}\n",
        "pd.DataFrame(data_small).to_parquet('small_dataset.parquet')\n",
        "\n",
        "# --- 2. Large Dataset (5M rows) ---\n",
        "print(f\"Creating Large Dataset ({num_rows_big:,} rows)...\")\n",
        "# This simulates a raw data dump (e.g., from S3)\n",
        "data_big = {\n",
        "    'id': range(num_rows_big),\n",
        "    'val': np.random.rand(num_rows_big),\n",
        "    'text': [f\"CONFIDENTIAL: Project Alpha data {i} Contact: user{i}@corp.com\" for i in range(num_rows_big)]\n",
        "}\n",
        "pd.DataFrame(data_big).to_parquet('large_dataset.parquet')\n",
        "\n",
        "# Cleanup memory to be safe on T4\n",
        "del data_small, data_big\n",
        "gc.collect()\n",
        "\n",
        "print(\"Data generation complete.\")"
      ],
      "metadata": {
        "id": "HXOgxvJcHalO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. The GPU Pipeline: Ingest, Clean, Dedup**\n",
        "The Zero-Copy Architecture: Here is the core advantage. In a standard pipeline, you load data to RAM, process it on CPU, then copy it to GPU for the LLM. That copy over the PCIe bus is a massive bottleneck.\n",
        "\n",
        "In this pipeline:\n",
        "\n",
        "- Ingest: cudf loads directly to VRAM.\n",
        "\n",
        "- Clean: We use vectorized Regex (executing on thousands of CUDA cores) to redact PII.\n",
        "\n",
        "- Dedup: We hash and filter instantly in VRAM."
      ],
      "metadata": {
        "id": "f0s6qFa_X7jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Run GPU Pipeline (Ingest -> Clean -> Dedup)\n",
        "\n",
        "print(\"--- 1. Ingestion Benchmark (5M Rows) ---\")\n",
        "# Benchmark CPU\n",
        "start = time.time()\n",
        "_ = pd.read_parquet('large_dataset.parquet')\n",
        "print(f\"CPU Load Time: {time.time() - start:.2f}s\")\n",
        "\n",
        "# Benchmark GPU\n",
        "# (Warmup included in first call overhead, but usually negligible at this scale)\n",
        "start = time.time()\n",
        "gdf = cudf.read_parquet('large_dataset.parquet')\n",
        "print(f\"GPU Load Time: {time.time() - start:.2f}s\")\n",
        "\n",
        "print(\"\\n--- 2. High-Speed Cleaning (Regex) ---\")\n",
        "# Scenario: Redact all email addresses\n",
        "# cuDF compiles this regex state machine and runs it in parallel\n",
        "start = time.time()\n",
        "gdf['clean_text'] = gdf['text'].str.replace(\n",
        "    r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
        "    '<REDACTED>',\n",
        "    regex=True\n",
        ")\n",
        "print(f\"Regex Time: {time.time() - start:.2f}s\")\n",
        "print(f\"Sample: {gdf['clean_text'][0]}\")\n",
        "\n",
        "print(\"\\n--- 3. Deduplication ---\")\n",
        "# Exact deduplication via hashing\n",
        "start = time.time()\n",
        "gdf['hash'] = gdf['clean_text'].hash_values()\n",
        "gdf = gdf.drop_duplicates(subset=['hash'])\n",
        "print(f\"Dedup Time: {time.time() - start:.2f}s\")"
      ],
      "metadata": {
        "id": "a-LmoKzkHfIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Vector Search**\n",
        "The final step is Indexing.\n",
        "\n",
        "**Pro Tip:** We wrap the output in cp.asarray(). This tells CuPy (the GPU-accelerated NumPy equivalent) how to interpret that pointer, allowing us to safely transfer the results back to the CPU for the final display (cuDF -> PyTorch -> cuVS -> CuPy)."
      ],
      "metadata": {
        "id": "aT1fYTMdYah3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Vector Search (With Interoperability Fix)\n",
        "\n",
        "print(\"--- Embedding & Indexing ---\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "\n",
        "# 1. Embed a subset (10k docs)\n",
        "# We take a subset to keep the embedding time short for this demo\n",
        "subset_size = 10_000\n",
        "subset_texts = gdf['clean_text'].iloc[:subset_size].to_arrow().to_pylist()\n",
        "print(f\"Embedding {subset_size} docs...\")\n",
        "\n",
        "# Generate Embeddings (PyTorch Tensor on GPU)\n",
        "embeddings = model.encode(subset_texts, convert_to_tensor=True)\n",
        "\n",
        "# 2. Build Index\n",
        "# Convert PyTorch Tensor -> CuPy Array (Zero-Copy Handoff)\n",
        "embeddings_cp = cp.asarray(embeddings.cpu().numpy(), dtype=cp.float32)\n",
        "\n",
        "print(\"Building Index (CAGRA)...\")\n",
        "build_params = cagra.IndexParams(metric=\"sqeuclidean\")\n",
        "index = cagra.build(build_params, embeddings_cp)\n",
        "\n",
        "# 3. Search\n",
        "query = \"project alpha confidential\"\n",
        "print(f\"Querying: '{query}'\")\n",
        "query_vec = model.encode([query])\n",
        "query_cp = cp.asarray(query_vec, dtype=cp.float32)\n",
        "\n",
        "search_params = cagra.SearchParams()\n",
        "distances, neighbors = cagra.search(search_params, index, query_cp, k=3)\n",
        "\n",
        "# --- THE INTEROP FIX ---\n",
        "# cuVS returns a raw 'device_ndarray'.\n",
        "# We wrap it in cp.asarray() so we can use standard .get() methods.\n",
        "neighbors_cp = cp.asarray(neighbors)\n",
        "indices = neighbors_cp.get().flatten() # Move only the results to CPU\n",
        "\n",
        "print(\"\\n--- Results ---\")\n",
        "for i, idx in enumerate(indices):\n",
        "    if idx < len(subset_texts):\n",
        "        print(f\"Match {i+1}: {subset_texts[idx]}\")"
      ],
      "metadata": {
        "id": "7aenKwEwHout"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}