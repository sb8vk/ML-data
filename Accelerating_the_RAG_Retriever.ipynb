{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "authorship_tag": "ABX9TyOSbWL2iSQgMwdRKkmZTHTt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sb8vk/ML/blob/master/Accelerating_the_RAG_Retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**The context**\n",
        "Most RAG discussions focus heavily on the LLM, but the real bottleneck for enterprise applications is often the Retriever (specifically the ingestion, cleaning, and indexing of massive datasets). If a standard CPU-based pipeline takes 12 hours to re-index your proprietary data, your LLM is effectively always 12 hours out of date. With NVIDIA RAPIDS, you can move the entire ETL lifecycle onto the GPU to enable near real-time updates.\n",
        "\n"
      ],
      "metadata": {
        "id": "WVL1l5pexznJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traditional approach:\n",
        "embeddings = model.encode(docs)  # GPU â†’ CPU copy\n",
        "index.add(embeddings)             # CPU â†’ GPU copy\n",
        "results = index.search(query)     # GPU â†’ CPU copy"
      ],
      "metadata": {
        "id": "wahcKRjAzjSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-copy approach:\n",
        "embeddings_gpu = model.encode(docs, convert_to_numpy=False)  # stays on GPU\n",
        "cudf_df['embeddings'] = embeddings_gpu                       # GPU-native DataFrame\n",
        "index = cuVS.build(cudf_df['embeddings'])                   # no data movement"
      ],
      "metadata": {
        "id": "e_O-mUKmzqGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**The approach**\n",
        " We are going to move the \"ETL\" (Extract, Transform, Load) part of RAG onto the GPU using NVIDIA RAPIDS. By keeping data on the GPU, we avoid the latency penalty of moving data back and forth between system RAM and VRAM."
      ],
      "metadata": {
        "id": "5Ud3FesAj9NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**How it works**\n",
        "We replace standard CPU tools with a \"Zero-Copy\" GPU pipeline. Data is loaded directly into VRAM and stays there through cleaning, deduplication, and indexing, avoiding the massive latency penalty of moving data back and forth between system RAM and GPU memory."
      ],
      "metadata": {
        "id": "kWDC3irkkGPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hardware Requirement**\n",
        "Setting up a GPU environment on cloud notebooks can be tricky. Three moving parts that must align:\n",
        "- The NVIDIA Driver (managed by Google)\n",
        "- The CUDA Runtime\n",
        "- The Python Version\n",
        "\n",
        "This script detects your specific environment configuration at runtime and fetches the correct pre-compiled wheels automatically. It handles the version matrix for you, ensuring a stable environment without manual troubleshooting."
      ],
      "metadata": {
        "id": "nG3zVVwFyCf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hguXWLlyVgGk",
        "outputId": "f5bc8e90-24f5-44d8-bab6-6fa6d98d2c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan 30 22:17:28 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STOP! Please go to 'Runtime > Restart Session' now.\")\n",
        "print(\"This is required to load the new CUDA libraries correctly.\")\n",
        "print(\"Then, proceed directly to Cell 2.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "qatCVsRLHJpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c564ea0e-6119-4bbe-b9b7-cbe2c1c2b901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 625, done.\u001b[K\n",
            "remote: Counting objects: 100% (191/191), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "fetch-pack: unexpected disconnect while reading sideband packet\n",
            "^C\n",
            "python3: can't open file '/content/rapidsai-csp-utils/colab/pip-install.py': [Errno 2] No such file or directory\n",
            "\n",
            "================================================================================\n",
            "STOP! Please go to 'Runtime > Restart Session' now.\n",
            "This is required to load the new CUDA libraries correctly.\n",
            "Then, proceed directly to Cell 2.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we installed low-level system libraries in Step 1, we had to restart the Python kernel. This gives us a clean slate but wipes out ephemeral packages. Here, we quickly restore sentence-transformers and import our GPU stack (cudf, cupy, cuvs)."
      ],
      "metadata": {
        "id": "0ektTlkUMV9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Restore Dependencies & Import**"
      ],
      "metadata": {
        "id": "6RlDAGqsN_P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import gc\n",
        "\n",
        "# 1. Restore Sentence-Transformers (often wiped on reset)\n",
        "try:\n",
        "    import sentence_transformers\n",
        "except ImportError:\n",
        "    print(\"Installing sentence-transformers...\")\n",
        "    !pip install -q sentence-transformers\n",
        "\n",
        "# 2. Import the RAPIDS Stack\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from cuvs.neighbors import cagra\n",
        "\n",
        "print(f\"   Environment Ready!\")\n",
        "print(f\"   RAPIDS cuDF Version: {cudf.__version__}\")\n",
        "print(f\"   GPU Detected: {cp.cuda.runtime.getDeviceCount()} device(s)\")"
      ],
      "metadata": {
        "id": "D4_Nz-FOHSor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1dfb9d3-2978-4e3e-ac0b-fcd87dc73694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Environment Ready!\n",
            "   RAPIDS cuDF Version: 25.02.01\n",
            "   GPU Detected: 1 device(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Data Generation: Latency vs. Throughput**\n",
        "A common misconception is that GPUs are always faster than CPUs for every task. In reality, GPUs have a fixed startup cost (initializing the CUDA context). For small data (e.g., 100k rows), the CPU often wins because it starts instantly. For big data (e.g., 5M rows), the GPU's massive parallelism amortizes that startup cost and dominates on throughput.\n",
        "\n",
        "We generate both \"Small\" and \"Large\" datasets to explicitly demonstrate this crossover point."
      ],
      "metadata": {
        "id": "GA86f5Uwpns4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sizes\n",
        "num_rows_small = 100_000\n",
        "num_rows_big   = 5_000_000\n",
        "\n",
        "print(f\"--- Generating Datasets ---\")\n",
        "\n",
        "# --- 1. Small Dataset (100k rows) ---\n",
        "print(f\"Creating Small Dataset ({num_rows_small:,} rows)...\")\n",
        "# We use a simple dictionary structure for speed\n",
        "data_small = {\n",
        "    'id': range(num_rows_small),\n",
        "    'val': np.random.rand(num_rows_small),\n",
        "    'text': [f\"CONFIDENTIAL: Project Alpha data {i} Contact: user{i}@corp.com\" for i in range(num_rows_small)]\n",
        "}\n",
        "pd.DataFrame(data_small).to_parquet('small_dataset.parquet')\n",
        "\n",
        "# --- 2. Large Dataset (5M rows) ---\n",
        "print(f\"Creating Large Dataset ({num_rows_big:,} rows)...\")\n",
        "# This simulates a raw data dump (e.g., from S3)\n",
        "data_big = {\n",
        "    'id': range(num_rows_big),\n",
        "    'val': np.random.rand(num_rows_big),\n",
        "    'text': [f\"CONFIDENTIAL: Project Alpha data {i} Contact: user{i}@corp.com\" for i in range(num_rows_big)]\n",
        "}\n",
        "pd.DataFrame(data_big).to_parquet('large_dataset.parquet')\n",
        "\n",
        "# Cleanup memory to be safe on T4\n",
        "del data_small, data_big\n",
        "gc.collect()\n",
        "\n",
        "print(\"Data generation complete.\")"
      ],
      "metadata": {
        "id": "HXOgxvJcHalO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebaeccf-ed1c-46b9-a70b-6b08aefc2628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Datasets ---\n",
            "Creating Small Dataset (100,000 rows)...\n",
            "Creating Large Dataset (5,000,000 rows)...\n",
            "Data generation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4: cudf & Vectorized Regex: Accelerate ETL at Scale**\n",
        "In a standard RAG pipeline, the copy operation over the PCIe bus is a massive bottleneck. Furthermore, text cleaning tasks like PII redaction often rely on slow, single-threaded CPU loops.\n",
        "\n",
        "In this pipeline:\n",
        "\n",
        "- Ingest: cudf loads directly to VRAM.\n",
        "\n",
        "- Clean: We use vectorized Regex (executing on thousands of CUDA cores) to redact PII.\n",
        "\n",
        "- Dedup: We hash and filter instantly in VRAM."
      ],
      "metadata": {
        "id": "f0s6qFa_X7jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import cudf\n",
        "import numpy as np\n",
        "\n",
        "def run_benchmark(file_path, label):\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"BENCHMARK: {label}\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    # --- 1. CPU (Pandas) Baseline ---\n",
        "    # We must measure this to prove the \"20s\" claim is real.\n",
        "    print(\">> Running CPU (Pandas)...\")\n",
        "    start_cpu = time.time()\n",
        "\n",
        "    # Load\n",
        "    t0 = time.time()\n",
        "    pdf = pd.read_parquet(file_path)\n",
        "    t_load_cpu = time.time() - t0\n",
        "\n",
        "    # Regex (The Bottleneck)\n",
        "    t0 = time.time()\n",
        "    pdf['clean_text'] = pdf['text'].str.replace(\n",
        "        r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
        "        '<REDACTED>',\n",
        "        regex=True\n",
        "    )\n",
        "    t_regex_cpu = time.time() - t0\n",
        "\n",
        "    # Dedup\n",
        "    t0 = time.time()\n",
        "    # Pandas doesn't support hash_values() directly; drop_duplicates is the standard equivalent\n",
        "    pdf = pdf.drop_duplicates(subset=['clean_text'])\n",
        "    t_dedup_cpu = time.time() - t0\n",
        "\n",
        "    total_cpu = time.time() - start_cpu\n",
        "    print(f\"   [CPU] Total: {total_cpu:.4f}s | Load: {t_load_cpu:.2f}s | Regex: {t_regex_cpu:.2f}s | Dedup: {t_dedup_cpu:.2f}s\")\n",
        "\n",
        "\n",
        "    # --- 2. GPU (cuDF) Accelerated ---\n",
        "    print(\">> Running GPU (cuDF)...\")\n",
        "    start_gpu = time.time()\n",
        "\n",
        "    # Load\n",
        "    t0 = time.time()\n",
        "    gdf = cudf.read_parquet(file_path)\n",
        "    t_load_gpu = time.time() - t0\n",
        "\n",
        "    # Regex (SIMT Execution)\n",
        "    t0 = time.time()\n",
        "    gdf['clean_text'] = gdf['text'].str.replace(\n",
        "        r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
        "        '<REDACTED>',\n",
        "        regex=True\n",
        "    )\n",
        "    t_regex_gpu = time.time() - t0\n",
        "\n",
        "    # Dedup (Hash-based)\n",
        "    t0 = time.time()\n",
        "    gdf['hash'] = gdf['clean_text'].hash_values()\n",
        "    gdf = gdf.drop_duplicates(subset=['hash'])\n",
        "    t_dedup_gpu = time.time() - t0\n",
        "\n",
        "    total_gpu = time.time() - start_gpu\n",
        "    print(f\"   [GPU] Total: {total_gpu:.4f}s | Load: {t_load_gpu:.2f}s | Regex: {t_regex_gpu:.2f}s | Dedup: {t_dedup_gpu:.2f}s\")\n",
        "\n",
        "    # --- Summary ---\n",
        "    speedup = total_cpu / total_gpu\n",
        "    if total_cpu < total_gpu:\n",
        "        print(f\"\\n RESULT: CPU was {total_gpu / total_cpu:.2f}x FASTER (Startup Overhead Dominates)\")\n",
        "    else:\n",
        "        print(f\"\\n RESULT: GPU was {speedup:.2f}x FASTER (Parallelism Dominates)\")\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# --- EXECUTE THE COMPARISON ---\n",
        "# 1. The Small Data Test (Proving the crossover point)\n",
        "_ = run_benchmark('small_dataset.parquet', \"Small Data (100k Rows)\")\n",
        "\n",
        "# 2. The Big Data Test (Proving the Scale)\n",
        "final_gdf = run_benchmark('large_dataset.parquet', \"Large Data (5M Rows)\")"
      ],
      "metadata": {
        "id": "a-LmoKzkHfIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e51959-0d15-48f7-ec72-637df767c923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "BENCHMARK: Small Data (100k Rows)\n",
            "========================================\n",
            ">> Running CPU (Pandas)...\n",
            "   [CPU] Total: 0.2872s | Load: 0.03s | Regex: 0.23s | Dedup: 0.02s\n",
            ">> Running GPU (cuDF)...\n",
            "   [GPU] Total: 0.1433s | Load: 0.12s | Regex: 0.02s | Dedup: 0.00s\n",
            "\n",
            " RESULT: GPU was 2.00x FASTER (Parallelism Dominates)\n",
            "\n",
            "========================================\n",
            "BENCHMARK: Large Data (5M Rows)\n",
            "========================================\n",
            ">> Running CPU (Pandas)...\n",
            "   [CPU] Total: 20.1099s | Load: 3.14s | Regex: 13.25s | Dedup: 3.72s\n",
            ">> Running GPU (cuDF)...\n",
            "   [GPU] Total: 0.8690s | Load: 0.29s | Regex: 0.53s | Dedup: 0.05s\n",
            "\n",
            " RESULT: GPU was 23.14x FASTER (Parallelism Dominates)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Vector Search**\n",
        "The final step is Indexing. We use cp.asarray() to wrap the raw device pointers returned by cuVS, telling CuPy (the GPU-accelerated NumPy equivalent) how to interpret the memory. This allows for a seamless handoff: cuDF (Dataframe) â†’ PyTorch (Tensor) â†’ cuVS (Index) â†’ CuPy (Result), ensuring high performance and ease of use."
      ],
      "metadata": {
        "id": "aT1fYTMdYah3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cupy as cp\n",
        "from cupy import from_dlpack\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# SAFETY CHECK: Import CAGRA safely\n",
        "try:\n",
        "    from cuvs.neighbors import cagra\n",
        "except ImportError:\n",
        "    # Fallback for older environments\n",
        "    from cuml.neighbors import cagra\n",
        "\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(\"PIPELINE: Zero-Copy Vector Indexing\")\n",
        "print(f\"{'='*40}\")\n",
        "\n",
        "# 0. Safety Check for Previous Cell\n",
        "if 'final_gdf' not in locals():\n",
        "    raise ValueError(\"ðŸš¨ variable 'final_gdf' is missing. Please run the Benchmark Cell (Cell 4) completely first!\")\n",
        "\n",
        "# Setup Model on GPU\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "\n",
        "# Take a subset\n",
        "subset_size = 50_000\n",
        "subset_texts = final_gdf['clean_text'].iloc[:subset_size].to_arrow().to_pylist()\n",
        "print(f\"Embedding {subset_size} documents...\")\n",
        "\n",
        "# 1. Generate Embeddings (PyTorch Tensor on VRAM)\n",
        "# CRITICAL FIX: normalize_embeddings=True ensures Euclidean distance == Cosine Similarity\n",
        "embeddings_torch = model.encode(subset_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
        "\n",
        "# 2. THE ZERO-COPY HANDOFF (DLPack)\n",
        "print(\">> Handoff: PyTorch Tensor -> CuPy Array (via DLPack)\")\n",
        "embeddings_cupy = from_dlpack(torch.utils.dlpack.to_dlpack(embeddings_torch))\n",
        "\n",
        "# 3. Build Index (CAGRA)\n",
        "print(\">> Building CAGRA Index...\")\n",
        "# We use sqeuclidean because we normalized the vectors above.\n",
        "build_params = cagra.IndexParams(metric=\"sqeuclidean\")\n",
        "index = cagra.build(build_params, embeddings_cupy)\n",
        "\n",
        "# 4. Search\n",
        "query = \"project alpha confidential\"\n",
        "print(f\"\\nQuerying: '{query}'\")\n",
        "# Normalize the query too!\n",
        "query_vec = model.encode([query], convert_to_tensor=True, normalize_embeddings=True)\n",
        "query_cupy = from_dlpack(torch.utils.dlpack.to_dlpack(query_vec))\n",
        "\n",
        "search_params = cagra.SearchParams()\n",
        "distances, neighbors = cagra.search(search_params, index, query_cupy, k=3)\n",
        "\n",
        "# 5. Extract Results\n",
        "# Wrap the device_ndarray in cp.asarray to access .get()\n",
        "final_indices = cp.asarray(neighbors).get().flatten()\n",
        "\n",
        "print(\"\\n--- Top Matches ---\")\n",
        "for i, idx in enumerate(final_indices):\n",
        "    if idx < len(subset_texts):\n",
        "        print(f\"[{i+1}] {subset_texts[idx]}\")"
      ],
      "metadata": {
        "id": "7aenKwEwHout",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd5b9b0-7a84-4843-e695-10bfb72ae2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "PIPELINE: Zero-Copy Vector Indexing\n",
            "========================================\n",
            "Embedding 50000 documents...\n",
            ">> Handoff: PyTorch Tensor -> CuPy Array (via DLPack)\n",
            ">> Building CAGRA Index...\n",
            "\n",
            "Querying: 'project alpha confidential'\n",
            "\n",
            "--- Top Matches ---\n",
            "[1] CONFIDENTIAL: Project Alpha data 978 Contact: <REDACTED>\n",
            "[2] CONFIDENTIAL: Project Alpha data 2025 Contact: <REDACTED>\n",
            "[3] CONFIDENTIAL: Project Alpha data 2027 Contact: <REDACTED>\n"
          ]
        }
      ]
    }
  ]
}